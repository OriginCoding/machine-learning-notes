{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabre-code/machine-learning-notes/blob/master/15_P2_Charater_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJS6ktV2T696",
        "outputId": "38edf6b3-d5e1-4c25-e86e-814ef719b5a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Length:  1130711\n",
            "Unique Characters: 85\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Reading Data\n",
        "with open(\"1268-0.txt\", \"r\", encoding=\"utf8\") as fp:\n",
        "    text = fp.read()\n",
        "\n",
        "start_indx = text.find(\"THE MYSTERIOUS ISLAND\")\n",
        "end_indx = text.find('End of the Project Gutenberg')\n",
        "text = text[start_indx:end_indx]\n",
        "char_set = set(text)\n",
        "\n",
        "print('Total Length: ', len(text))\n",
        "print('Unique Characters:', len(char_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd96j5oIT6-F",
        "outputId": "ce28ea9e-c738-4654-b623-0c06e2be73b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text encoded shape: (1130711,)\n",
            "THE MYSTERIOUS  == Encoding ==> [48 36 33  1 41 53 47 48 33 46 37 43 49 47  1]\n",
            "[37 47 40 29 42 32] == Reverse ==> ISLAND\n"
          ]
        }
      ],
      "source": [
        "chars_sorted = sorted(char_set)\n",
        "char2int = {ch:i for i, ch in enumerate(chars_sorted)}\n",
        "char_array = np.array(chars_sorted)\n",
        "text_encoded = np.array(\n",
        "    [char2int[ch] for ch in text],\n",
        "    dtype=np.int32\n",
        ")\n",
        "\n",
        "print('Text encoded shape:', text_encoded.shape)\n",
        "\n",
        "print(text[:15], '== Encoding ==>', text_encoded[:15])\n",
        "print(text_encoded[15:21], '== Reverse ==>',''.join(char_array[text_encoded[15:21]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUEQ2v5ZT6-H",
        "outputId": "71adc078-9b06-4544-c50e-67cfbc2991b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48 -> T\n",
            "36 -> H\n",
            "33 -> E\n",
            "1 ->  \n",
            "41 -> M\n"
          ]
        }
      ],
      "source": [
        "for ex in text_encoded[:5]:\n",
        "    print('{} -> {}'.format(ex, char_array[ex]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlVsSeeZT6-I",
        "outputId": "60477417-bd97-4d47-a63b-557881025177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[48 36 33  1 41 53 47 48 33 46 37 43 49 47  1 37 47 40 29 42 32  1 10 10\n",
            " 10  0  0  0  0  0 48 36 33  1 41 53 47 48 33 46]  ->  37\n",
            "'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTER'  ->  'I'\n"
          ]
        }
      ],
      "source": [
        "seq_length = 40\n",
        "chunk_size = seq_length + 1\n",
        "\n",
        "text_chunks = [text_encoded[i:i+chunk_size]\n",
        "               for i in range(len(text_encoded)-chunk_size+1)]\n",
        "\n",
        "## inspection:\n",
        "for seq in text_chunks[:1]:\n",
        "    input_seq = seq[:seq_length]\n",
        "    target = seq[seq_length]\n",
        "    print(input_seq, ' -> ', target)\n",
        "    print(repr(''.join(char_array[input_seq])),\n",
        "          ' -> ', repr(''.join(char_array[target])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5f9njUPlT6-L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
        "\n",
        "seq_dataset = TextDataset(torch.tensor(np.array(text_chunks)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1ctFTd_T6-N",
        "outputId": "73b44c69-9afa-49e5-81a7-60ee40251083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Input (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTER'\n",
            "Target (y): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
            "\n",
            " Input (x): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
            "Target (y): 'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERIO'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, (seq, target) in enumerate(seq_dataset):\n",
        "    print(' Input (x):', repr(''.join(char_array[seq])))\n",
        "    print('Target (y):', repr(''.join(char_array[target])))\n",
        "    print()\n",
        "    if i == 1:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G-YGXC6CT6-O"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "# device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xTL9lMoPT6-Q"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "torch.manual_seed(1)\n",
        "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tj_xwv5T6-R"
      },
      "source": [
        "Building a character-level RNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpGiQ41NT6-V",
        "outputId": "3d5dc360-ac51-4bed-f5ae-8d410b959289"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(85, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=85, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x).unsqueeze(1)\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        return hidden.to(device), cell.to(device)\n",
        "\n",
        "vocab_size = len(char_array)\n",
        "embed_dim = 256\n",
        "rnn_hidden_size = 512\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "model = model.to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HggbVhwBT6-X",
        "outputId": "b2bf971e-3474-4b70-b879-b4ef07931a9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 loss: 4.4364\n",
            "Epoch 500 loss: 1.3653\n",
            "Epoch 1000 loss: 1.2382\n",
            "Epoch 1500 loss: 1.2409\n",
            "Epoch 2000 loss: 1.2136\n",
            "Epoch 2500 loss: 1.1760\n",
            "Epoch 3000 loss: 1.2151\n",
            "Epoch 3500 loss: 1.1541\n",
            "Epoch 4000 loss: 1.1780\n",
            "Epoch 4500 loss: 1.1347\n",
            "Epoch 5000 loss: 1.1322\n",
            "Epoch 5500 loss: 1.1497\n",
            "Epoch 6000 loss: 1.1423\n",
            "Epoch 6500 loss: 1.1008\n",
            "Epoch 7000 loss: 1.1575\n",
            "Epoch 7500 loss: 1.1695\n",
            "Epoch 8000 loss: 1.1344\n",
            "Epoch 8500 loss: 1.0815\n",
            "Epoch 9000 loss: 1.1400\n",
            "Epoch 9500 loss: 1.1633\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "num_epochs = 10000\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    hidden, cell = model.init_hidden(batch_size)\n",
        "    seq_batch, target_batch = next(iter(seq_dl))\n",
        "    seq_batch = seq_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    for c in range(seq_length):\n",
        "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
        "        loss += loss_fn(pred, target_batch[:, c])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss = loss.item()/seq_length\n",
        "    if epoch % 500 == 0:\n",
        "        print(f'Epoch {epoch} loss: {loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ0uemLST6-Y"
      },
      "source": [
        "Evaluation phase: generating new text passages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8Slbm4cT6-Y",
        "outputId": "9ecd51cd-34a6-4269-f55c-790ae98a14e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilities: [0.33333334 0.33333334 0.33333334]\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [1]\n",
            " [1]]\n"
          ]
        }
      ],
      "source": [
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "logits = torch.tensor([[1.0, 1.0, 1.0]])\n",
        "\n",
        "print('Probabilities:', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
        "\n",
        "m = Categorical(logits=logits)\n",
        "samples = m.sample((10,))\n",
        "\n",
        "print(samples.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmhCLpOWT6-Z",
        "outputId": "d6c6b4f1-211d-432c-8433-20588d6dc5cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilities: [0.10650698 0.10650698 0.78698605]\n",
            "[[0]\n",
            " [2]\n",
            " [2]\n",
            " [1]\n",
            " [2]\n",
            " [1]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]]\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
        "\n",
        "print('Probabilities:', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
        "\n",
        "m = Categorical(logits=logits)\n",
        "samples = m.sample((10,))\n",
        "\n",
        "print(samples.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0BP9qsVT6-Z",
        "outputId": "99a5073a-ecf6-4bb1-b9bb-b825a29258b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The island is thought of its own work or green artures away there on the river, and you boundly birds, the wind was situated during the extremity.\n",
            "The jagged reply closed, and was nearly everything caught for this absence were cast remaining. Neipable dogners,” said Pencroft, as Pencroft of trees, therefore, Top was harding always\n",
            "succeeded. There is\n",
            "ricated knows, and he plungs after was still.\n",
            "There they must became motion.\n",
            "\n",
            "No runkles searched from returning it deasts had at give you at the portrary, a\n"
          ]
        }
      ],
      "source": [
        "def sample(model, starting_str,\n",
        "           len_generated_text=500,\n",
        "           scale_factor=1.0):\n",
        "\n",
        "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
        "\n",
        "    generated_str = starting_str\n",
        "\n",
        "    model.eval()\n",
        "    hidden, cell = model.init_hidden(1)\n",
        "    hidden = hidden.to('cpu')\n",
        "    cell = cell.to('cpu')\n",
        "    for c in range(len(starting_str)-1):\n",
        "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n",
        "\n",
        "    last_char = encoded_input[:, -1]\n",
        "    for i in range(len_generated_text):\n",
        "        logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
        "        logits = torch.squeeze(logits, 0)\n",
        "        scaled_logits = logits * scale_factor\n",
        "        m = Categorical(logits=scaled_logits)\n",
        "        last_char = m.sample()\n",
        "        generated_str += str(char_array[last_char])\n",
        "\n",
        "    return generated_str\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model.to('cpu')\n",
        "print(sample(model, starting_str='The island'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R7YgwKlT6-a",
        "outputId": "9ba36328-750b-4c79-9758-b62e2cd3a2b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilities before scaling:         [0.10650698 0.10650698 0.78698605]\n",
            "Probabilities after scaling with 0.5: [0.21194156 0.21194156 0.57611686]\n",
            "Probabilities after scaling with 0.1: [0.3104238  0.3104238  0.37915248]\n"
          ]
        }
      ],
      "source": [
        "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
        "\n",
        "print('Probabilities before scaling:        ', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
        "\n",
        "print('Probabilities after scaling with 0.5:', nn.functional.softmax(0.5*logits, dim=1).numpy()[0])\n",
        "\n",
        "print('Probabilities after scaling with 0.1:', nn.functional.softmax(0.1*logits, dim=1).numpy()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWPPt3WCT6-a",
        "outputId": "0caebaa3-e6ed-46ac-c328-dad823b41a56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The island is the current of the sailor did not be uttered at the engineer, and the sailor was provided themselves by the plateau, which would find them.\n",
            "\n",
            "“The colonists and would not have been in discover them in a sort of the colonists of the mouth of the Falls River, and without succeedered by the reporter, and the reporter then unexpectedly a seals of the coast, and the reporter only only without hunger, which were besides, and they had not at the reporter and Pencroft.\n",
            "\n",
            "“We must be always that he had\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "print(sample(model, starting_str='The island',\n",
        "             scale_factor=2.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi4KOqv2T6-b",
        "outputId": "f30d887e-57ba-4a03-9281-2a6f1ecc3014"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The island ifute,\n",
            "cut oae heor Cappe o regaplical undulated wanash, while droce obundakd. Everythirg,\n",
            "nut down no suspicis plefuct! I hope.n vovictors asked,\n",
            "Pencroft, ftrasswells,” hur!”\n",
            " sawts of chucc,--olpoint. As\n",
            "nests off degreers, 5th. Nemp. Eamptine FeE.s\n",
            "Ligascij., I’s Beejoud, risiny only joicated,” said, hearding!\n",
            "Granimn thra”--Femb.” ED!\n",
            "But” he knews, know-boat-pHute woodwarks only?--Arsponjermed hours now--yzants,\n",
            "druterouss a=Le,\n",
            "1-zah rerthus. But\n",
            "dea!.” In\n",
            "satcrivily,’ ’law, Herberhel, a\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "print(sample(model, starting_str='The island',\n",
        "             scale_factor=0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gwxR4xMkZIlk"
      },
      "outputs": [],
      "source": [
        "torch.save(model,\"lstm2.ph\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
